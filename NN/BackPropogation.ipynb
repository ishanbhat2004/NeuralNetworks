{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a86a0604-bf59-4dbc-8afe-7c1ee8ab7833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, Loss: 36.0\n",
      "Iteration 2, Loss: 33.872400000000006\n",
      "Iteration 3, Loss: 31.870541159999995\n",
      "Iteration 4, Loss: 29.98699217744401\n",
      "Iteration 5, Loss: 28.21476093975706\n",
      "Iteration 6, Loss: 26.54726856821742\n",
      "Iteration 7, Loss: 24.978324995835766\n",
      "Iteration 8, Loss: 23.50210598858187\n",
      "Iteration 9, Loss: 22.113131524656684\n",
      "Iteration 10, Loss: 20.80624545154948\n",
      "Iteration 11, Loss: 19.576596345362915\n",
      "Iteration 12, Loss: 18.419619501351963\n",
      "Iteration 13, Loss: 17.331019988822057\n",
      "Iteration 14, Loss: 16.306756707482677\n",
      "Iteration 15, Loss: 15.34302738607045\n",
      "Iteration 16, Loss: 14.436254467553686\n",
      "Iteration 17, Loss: 13.58307182852126\n",
      "Iteration 18, Loss: 12.780312283455652\n",
      "Iteration 19, Loss: 12.024995827503426\n",
      "Iteration 20, Loss: 11.314318574097975\n",
      "Final weights: [-3.18248226 -0.63503547  1.45255321]\n",
      "Final Bias: 0.8175177371706989\n"
     ]
    }
   ],
   "source": [
    "## Chain rule is the backbone of the backproppogation algorithm\n",
    "\n",
    "## how to optimize the weights and biases of a neuron so that the output is close to 0\n",
    "\n",
    "# For a single neuron, we need to find how to best optimize weights and biases such that the loss is minimized\n",
    "# i.e. we need to move in negative gradient direction - find the derivative of the loss w.r.t. weights and biases\n",
    "\n",
    "# Coding the Backpropogation algorithm\n",
    "import numpy as np\n",
    "\n",
    "def relu(input):\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def derivativeRelu(input):\n",
    "    return np.where(input > 0, 1, 0)\n",
    "\n",
    "weights = np.array([-3.0, -1.0, 2.0])  # Initialize as float\n",
    "bias = 1.0  # Initialize as float\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(20):\n",
    "    # Forward pass\n",
    "    output = np.dot(inputs, weights) + bias\n",
    "    relu_output = relu(output)\n",
    "    loss = (relu_output - target) ** 2\n",
    "\n",
    "    # Backward pass\n",
    "    dLoss_dReluOutput = 2 * (relu_output - target)\n",
    "    dReluOut_dOutput = derivativeRelu(output)\n",
    "    dOutput_dWeights = inputs\n",
    "    dOutput_dBias = 1\n",
    "\n",
    "    dloss_dlinear = dLoss_dReluOutput * dReluOut_dOutput\n",
    "    dloss_dweights = dloss_dlinear * dOutput_dWeights\n",
    "    dloss_dbias = dloss_dlinear * dOutput_dBias\n",
    "\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    # if i % 1000 == 0:  # Print loss every 1000 iterations\n",
    "    print(f\"Iteration {i + 1}, Loss: {loss}\")\n",
    "    \n",
    "print(f\"Final weights: {weights}\")\n",
    "print(f\"Final Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f86a8217-d7be-4823-96d0-ec03d0c94873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
