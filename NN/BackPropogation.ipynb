{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a0604-bf59-4dbc-8afe-7c1ee8ab7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain rule is the backbone of the backproppogation algorithm\n",
    "\n",
    "## how to optimize the weights and biases of a neuron so that the output is close to 0\n",
    "\n",
    "# For a single neuron, we need to find how to best optimize weights and biases such that the loss is minimized\n",
    "# i.e. we need to move in negative gradient direction - find the derivative of the loss w.r.t. weights and biases\n",
    "\n",
    "# Coding the Backpropogation algorithm\n",
    "import numpy as np\n",
    "\n",
    "def relu(input):\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def derivativeRelu(input):\n",
    "    return np.where(input > 0, 1, 0)\n",
    "\n",
    "weights = np.array([-3.0, -1.0, 2.0])  # Initialize as float\n",
    "bias = 1.0  # Initialize as float\n",
    "inputs = np.array([1.0, -2.0, 3.0])\n",
    "target = 0.0\n",
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(1000000):\n",
    "    # Forward pass\n",
    "    output = np.dot(inputs, weights) + bias\n",
    "    relu_output = relu(output)\n",
    "    loss = (relu_output - target) ** 2\n",
    "\n",
    "    # Backward pass\n",
    "    dLoss_dReluOutput = 2 * (relu_output - target)\n",
    "    dReluOut_dOutput = derivativeRelu(output)\n",
    "    dOutput_dWeights = inputs\n",
    "    dOutput_dBias = 1\n",
    "\n",
    "    dloss_dlinear = dLoss_dReluOutput * dReluOut_dOutput\n",
    "    dloss_dweights = dloss_dlinear * dOutput_dWeights\n",
    "    dloss_dbias = dloss_dlinear * dOutput_dBias\n",
    "\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    bias -= learning_rate * dloss_dbias\n",
    "\n",
    "    if i % 1000 == 0:  # Print loss every 1000 iterations\n",
    "        print(f\"Iteration {i + 1}, Loss: {loss}\")\n",
    "    \n",
    "print(f\"Final weights: {weights}\")\n",
    "print(f\"Final Bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86a8217-d7be-4823-96d0-ec03d0c94873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the ReLU function and its derivative\n",
    "def relu(input):\n",
    "    return np.maximum(0, input)\n",
    "\n",
    "def relu_derivative(input):\n",
    "    return np.where(input > 0, 1, 0)\n",
    "\n",
    "# Initialize inputs, weights, biases, and learning rate\n",
    "inputs = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "weights = np.array([[0.1, 0.2, 0.3, 0.4], \n",
    "                    [0.1, 0.2, 0.3, 0.4], \n",
    "                    [0.1, 0.2, 0.3, 0.4]])\n",
    "biases = np.array([1.0, 1.0, 1.0])\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Training loop\n",
    "for i in range(10000):\n",
    "    # Forward Pass\n",
    "    output = np.dot(inputs, weights.T) + biases\n",
    "    activation_output = relu(output)\n",
    "    y = np.sum(activation_output)\n",
    "    Loss = y ** 2\n",
    "\n",
    "    # Backward Pass\n",
    "    dloss_dy = 2 * y\n",
    "    dy_dcombineact = np.ones_like(activation_output)\n",
    "    dloss_dcombineact = dloss_dy * dy_dcombineact\n",
    "\n",
    "    dcombineact_drelu = relu_derivative(output)\n",
    "    dloss_drelu = dloss_dcombineact * dcombineact_drelu\n",
    "\n",
    "    # Compute gradients\n",
    "    dloss_dweights = np.outer(dloss_drelu, inputs)  # Gradient of weights\n",
    "    dloss_dbiases = dloss_drelu  # Gradient of biases\n",
    "\n",
    "    # Update weights and biases\n",
    "    weights -= learning_rate * dloss_dweights\n",
    "    biases -= learning_rate * dloss_dbiases\n",
    "\n",
    "    # Print loss every 20 iterations\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Iteration {i}: Loss = {Loss}\")\n",
    "\n",
    "# Print final weights and biases\n",
    "print(f\"Final Weights: {weights}\")\n",
    "print(f\"Final Biases: {biases}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc4c0de2-8564-4976-9715-32f70b05d801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "## hard coding the process of calculating the gradient matrix of loss wrt weights\n",
    "import numpy as np\n",
    "dvalues = np.array([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.]]) ## 3 by 3 matrix \n",
    "inputs = np.array([[1, 2, 3, 2.5], [2., 5., -1, 2], [-1.5, 2.7, 3.3, -0.8]])\n",
    "\n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caab217e-8d4e-4699-9a18-723e055abc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "## hard coding the process of calculating the gradient matrix of loss wrt biases\n",
    "dvalues = np.array([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.]]) ## 3 by 3 matrix \n",
    "\n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dd2c4bb-bf1a-4b97-a478-e9ffeee88484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "## hard coding the process of calculating the gradient matrix of loss wrt inputs \n",
    "dvalues = np.array([[1., 1., 1.], [2., 2., 2.], [3., 3., 3.]]) ## 3 by 3 matrix \n",
    "weights = np.array([[0.2, 0.8, -0.5, 1.], [0.5, -0.91, 0.26, -0.5], [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "dinputs = np.dot(dvalues, weights.T)\n",
    "print(dinputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e4f608d-15e8-42d6-8014-525bc707f87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, num_features, num_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(num_features, num_neurons)\n",
    "        self.bias = np.zeros((1, num_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis = 0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a03e7c28-139f-4ee3-a39d-161a00a6dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RELU Class\n",
    "class RELU: \n",
    "    def forward(self, inputs): \n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs) ### Output will have the same shape as the input\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b84760b-33d5-463e-99d9-0d97e016d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip predictions to avoid log(0) errors\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Negative log likelihood\n",
    "        negative_log = -np.log(correct_confidences)\n",
    "        return negative_log\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        if (len(y_true.shape) == 1):\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        self.dinputs = -y_true/dvalues\n",
    "        self.dinputs = self.dinputs/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac809c1-b42a-4784-bfb2-fdde94caaeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Softmax Activation Function\n",
    "import numpy as np\n",
    "class Softmax: \n",
    "    def forward(self, inputs): \n",
    "        ## Find the raw exponentials\n",
    "        raw_exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        ## Find the probabilistic measure \n",
    "        self.output = raw_exp / np.sum(raw_exp, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c587e1fc-4b57-42cb-b454-63e087c3d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    def __init__(self):\n",
    "        self.activation = Softmax()\n",
    "        self.loss = Loss_CategoricalEntropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues,y_true):\n",
    "        ## Here, dvalues is the predicted output\n",
    "        samples = len(dvalues)\n",
    "        if (len(y_true.shape) == 2):\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        ## Finding the gradients\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        ## Normalization\n",
    "        self.dinputs = self.dinputs/samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
