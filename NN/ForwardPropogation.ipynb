{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f8041-2c49-43f5-b08e-b4161e5e75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coding a single neuron with inputs and weights\n",
    "def neuron(inputs, weights, bias):\n",
    "    sum = 0\n",
    "    for pair in zip(inputs, weights):\n",
    "        a, b = pair\n",
    "        sum += a * b\n",
    "\n",
    "    sum += bias\n",
    "\n",
    "    return sum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633516c-af39-411d-bfc5-ae80041e46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 2, 3]\n",
    "weights = [0.2, 0.8, -0.5] ## correspond to the importance of each input\n",
    "\n",
    "print(neuron(inputs, weights, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c69b16-0d25-4df0-8032-d7d24520a807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implementing a layer of neurons \n",
    "def layer(inputs, weights, bias):\n",
    "    output = []\n",
    "    for i in range(len(weights)):\n",
    "        sum = 0\n",
    "        for j in range(len(weights[i])):\n",
    "            sum += weights[i][j] * inputs[j]\n",
    "        sum += bias[i]\n",
    "        output.append(sum)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3753fd-0e03-4df1-bcf2-8c9158c13b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37411ba7-33b8-4d01-a89f-1dd75f03f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [[0.5, 0.5, 0.5], [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]]\n",
    "\n",
    "print(layer(inputs,weights, [0, 0, 1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00294b8c-f1c7-4e2c-b8b7-1e4eecad474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Coding with numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4270f4c-cf17-4b96-b875-2b4a890b96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents a single neuron\n",
    "bias = 2\n",
    "outputs = np.dot(inputs, weights) + bias\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64fd95-c2ff-4e94-88d8-3d3cf460e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coding a layer\n",
    "\n",
    "weights = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "inputs = [1, 2, 3]\n",
    "bias = [1, 1, 1]\n",
    "\n",
    "outputs = np.dot(weights, inputs) + bias\n",
    "print(outputs.reshape(3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569433f6-01d4-464c-a6a0-4b3ce00ebcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing batches of data\n",
    "batch_data = [[1, 2, 3], [2, 2, 2], [1, 5, 4], [6, 4, 1]]\n",
    "weights = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
    "bias = [2, 2, 2]\n",
    "\n",
    "outputs = np.dot(batch_data, np.array(weights).T) + bias\n",
    "print(outputs.shape)\n",
    "\n",
    "## Shape of the Output must be (4, 3), where we have 4 batches of data and 3 outputs for each of the 3 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34782b85-be6b-442c-af9a-1b4a0c87b418",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating multiple layers - In this specific instance, we'll take 2 layers with 3 batches of data\n",
    "\n",
    "weights_2 = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]\n",
    "bias_2 = [2, 2, 2]\n",
    "\n",
    "final_output = np.dot(outputs, np.array(weights_2).T) + bias\n",
    "\n",
    "print(final_output.shape) ## For all batches of data, I need to have an output from each neuron in the layer -> shape = (no. of samples, no.of neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb7053-f909-485b-9c18-c6488a9e20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## generating data - spiral \n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt \n",
    "X, y = spiral_data(samples = 100, classes = 3) # X- data , y - class\n",
    "# plt.scatter(X[:, 0], X[:, 1])\n",
    "# plt.show()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8824d8b-0ab5-4be8-a6e0-95593eef600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seeing the different classes\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='brg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c7c9c7d-b8a5-4d37-a70b-72e930b6ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a class for Dense Neural layers\n",
    "## It takes the numer of inputs and the number of neurons for each layer\n",
    "import numpy as np\n",
    "class Layer_Dense:\n",
    "    def __init__(self, num_features, num_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(num_features, num_neurons)\n",
    "        self.bias = np.zeros((1, num_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791ea20-baf1-4e33-a945-a05b7e8a8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(2, 3)\n",
    "dense1.forward(X)\n",
    "print(dense1.output[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7594ac94-c14c-47a2-a90a-6f4479044ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12, 15, 18]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## learning the different sum commands in python \n",
    "import numpy as np\n",
    "A = [[1, 2, 3], [4, 5 , 6], [7,8, 9]]\n",
    "np.sum(A, axis = 0, keepdims=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be0bb9-47cc-47b8-bf36-0d857fb1ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Learning about broadcasting rules - Just check each dimension value, if it differs, then check if one of them is none or zero. If it is, then it is broadcastable.\n",
    "### The way we broadcast is to extend it along the dimension that it is short in. \n",
    "out = np.array(A) + np.array([[1, 2, 3]]).T\n",
    "\n",
    "print(np.array(out))\n",
    "\n",
    "## Practice - From each row in a 2x2 grid, remove the maximun element from each element of that row \n",
    "\n",
    "matrix = np.array([\n",
    "    [1, 5, 3],\n",
    "    [7, 2, 9],\n",
    "    [4, 6, 8]\n",
    "])\n",
    "\n",
    "max_in_rows = np.amax(matrix, axis=1, keepdims=True)\n",
    "out = matrix - max_in_rows\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d4ced3-27b2-4746-8d78-cfb6ae7f0e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation functions\n",
    "\n",
    "### The need of activation functions is mainly to represent the non-linearity in the data - This makes my network more expressive\n",
    "\n",
    "### Different kinds of activation functions: \n",
    "### 1.) RELU (Rectified Linear Unit) : It is a function that just takes the maximum between 0 and x (Different variants available)\n",
    "### 2.) Sigmoid Activation : Used to truncate the output between 0 and 1\n",
    "### 3.) Softmax Activation Function: Used in multi-class classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1c87e0-ebef-4b98-99a9-1e5895bdda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RELU Class\n",
    "class RELU: \n",
    "    def forward(self, inputs): \n",
    "        self.output = np.maximum(0, inputs) ### Output will have the same shape as the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63520d3f-c5cf-477b-b3cd-a898e352b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Softmax Activation Function\n",
    "import numpy as np\n",
    "class Softmax: \n",
    "    def forward(self, inputs): \n",
    "        ## Find the raw exponentials\n",
    "        raw_exp = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        ## Find the probabilistic measure \n",
    "        self.output = raw_exp / np.sum(raw_exp, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f1f17-0534-47c3-8259-52dfc0c137cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fcfc6b-2906-406b-93e1-3e88537dc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Coding a Forward Pass (Without Loss Function)\n",
    "\n",
    "### Steps: \n",
    "# 1) Define the number of layers you need in the networks along with the number of features in the feature set, also generate the data\n",
    "# 2) Now, you have to use the activation functions on the outputs of the Neurons - RELU, then Softmax\n",
    "# 3) Return the output from the Softmax function \n",
    "\n",
    "# generate the spiral data\n",
    "## generating data - spiral \n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt \n",
    "X, y = spiral_data(samples = 100, classes = 3) # X- data , y - class\n",
    "\n",
    "## Create the first Layer with 3 neurons\n",
    "Layer1 = Layer_Dense(2, 3)\n",
    "\n",
    "## Get the output from the first layer\n",
    "Layer1.forward(X) ## 300 x 3\n",
    "\n",
    "## Send he output from the first layer to the activation function \n",
    "relu_activator = RELU()\n",
    "relu_activator.forward(Layer1.output)\n",
    "\n",
    "## This now forms the input to the second Neural Network\n",
    "Layer2 = Layer_Dense(3, 3)\n",
    "\n",
    "## Generate the output from the second layer using the relu output\n",
    "Layer2.forward(relu_activator.output)\n",
    "\n",
    "## Finally, send this output to the softmax function\n",
    "softmax_activator = Softmax()\n",
    "softmax_activator.forward(Layer2.output)\n",
    "\n",
    "## Print the output for each of the data points\n",
    "print(softmax_activator.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1e494fa-e421-4f18-b293-7838936daa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12300237 0.37120515 0.26867129]\n",
      "[2.09555162 0.9910004  1.31426661]\n",
      "1.4669395423242937\n"
     ]
    }
   ],
   "source": [
    "## Now, we need a way to evaluate our predictions and tune our weights accordingly\n",
    "## Loss Functions \n",
    "# 1) Cross Entropy Loss - helps to analyze the predictions made during the classification problem = - Summation(True Label * log(predicted label))\n",
    "## Side Note: One-hot encoding -- This technique is used for representing categorical features, like the colors red, blue, green could be represented as [1, 0, 0] if the output is red\n",
    "\n",
    "Softmax_outputs = np.random.randn(3, 3)\n",
    "obj = Softmax()\n",
    "obj.forward(Softmax_outputs)\n",
    "check = obj.output\n",
    "\n",
    "class_target = [0, 1, 1]\n",
    "print(check[[0, 1, 2], class_target])\n",
    "\n",
    "# find the negative log of all the target class outputs\n",
    "neg_log = - np.log(check[range(len(check)), class_target])\n",
    "print(neg_log)\n",
    "\n",
    "## find the mean of all the data points to finally return the total loss\n",
    "avg = np.mean(neg_log)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61d4a877-9571-4ee2-8ec7-e2d41d26a6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4669395423242937\n"
     ]
    }
   ],
   "source": [
    "## Finding the output of the element wise multiplication\n",
    "y_true = np.array([\n",
    "    [1, 0, 0], \n",
    "    [0, 1, 0], \n",
    "    [0, 1, 0]\n",
    "])\n",
    "\n",
    "y_pred = check\n",
    "\n",
    "a = y_true * y_pred\n",
    "b = np.sum(a, axis = 1)\n",
    "c = - np.log(b)\n",
    "\n",
    "print(np.mean(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4275f0c4-9657-4ed6-8427-935da8e6f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip predictions to avoid log(0) errors\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            # Fix: Use samples instead of len(samples)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Negative log likelihood\n",
    "        negative_log = -np.log(correct_confidences)\n",
    "        return negative_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5abfeff-5ed7-48c7-9c19-bcef6e814576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0986104\n"
     ]
    }
   ],
   "source": [
    "## Coding forward pass from scratch \n",
    "\n",
    "## Data \n",
    "from nnfs.datasets import spiral_data\n",
    "import numpy as np\n",
    "import nnfs\n",
    "\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt \n",
    "X, y = spiral_data(samples = 100, classes = 3) # X- data , y - true class\n",
    "\n",
    "## Set up the network\n",
    "Layer_1 = Layer_Dense(2, 3)\n",
    "Layer_2 = Layer_Dense(3, 3)\n",
    "\n",
    "## Set up the peripheries - Softmax and categorical loss\n",
    "softmax = Softmax()\n",
    "relu = RELU()\n",
    "Categorical = Loss_CategoricalEntropy()\n",
    "\n",
    "Layer_1.forward(X)\n",
    "relu.forward(Layer_1.output)\n",
    "\n",
    "Layer_2.forward(relu.output)\n",
    "softmax.forward(Layer_2.output)\n",
    "\n",
    "loss = Categorical.calculate(softmax.output, y)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e82361ab-e5e1-4822-8f42-142b0abee1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "## Accuracy \n",
    "Softmax_outputs = np.random.randn(3, 3)\n",
    "obj = Softmax()\n",
    "obj.forward(Softmax_outputs)\n",
    "check = obj.output\n",
    "\n",
    "class_target = [0, 1, 1]\n",
    "\n",
    "predictions = np.argmax(check, axis = 1)\n",
    "\n",
    "if (len(class_target) == 2):\n",
    "    class_target = np.argmax(class_target, axis = 1)\n",
    "\n",
    "accuracy = np.mean(predictions == class_target)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8b9a5-f5aa-4fc8-aefa-b459831aa5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How do I update these weights and biases effectively. 2 naive solutions: random selection and update\n",
    "## Better Solution: Gradient Descent \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22798f-a8d9-43b7-bdd9-82d7379482f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
